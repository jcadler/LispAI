One of the main reasons that I have been interested in computers is their philisophical potential. Computers force us to ask intense philosohpical questions when tackling different problems. How do we think? How do we process information? How do we take in information? This is one of the reasons I tackled my AI project, the ability for a computer to think in the same as a human tantilized me and fascinated me. I began the summer looking for a project to do, because the summer before I had worked with a PIC microcontroller and loved it (if you are interested see the link on the left). I wanted something to do, but something more high level; a puzzle which I could attempt to solve. During the school year I had worked on an object for Microsoft's "Hunt the Wumpus" competition (link on the left) and had come up with an idea for, what I called a question getter. While you were playing it would access the wikipedia API and use the information gathered from articles to create trivia questions on the fly; you would never have the same game twice. I decided to take that idea and run with it, but instead of being restricted to C# (and all of its clunkiness) I could use whatever languages or tools that I wanted.

I already had most of the Wikipedia API code written (as I had begun the project during the competition) but had to finish it off. I spent a week or two working through the "infrastructure" code, as I like to call it, and getting things ready to begin working with the AI. Concurrently, I was working through various books on Common Lisp, starting with Land of Lisp by Conrad Barski and moving up to Practical Common Lisp by Peter Seibel. Previously, I received the idea to use Lisp from a friend of my father's, who was an ardent Lisp fan and supported its use for AI. After finishing off the C# code I spent a two week hiatus at a Stanford Speech camp. Afterwords, I had five weeks to finish off the project, so I immediately began working on how I was to implement the AI. Unfortunately I could not begin the AI code immediately, I had to spend another week writing infrastructure code for the Lisp end. Specifically I had to write code which would bring the articles (which were parsed and formatted by the C# "front end") and go through them and put them into lists of words, formatting them in a way which would be easy to handle. Over the next few weeks I spent the majority of my time trying to, alternatively, get my C# laptop and Lisp Desktop (which was running Ubuntu Linux) to communicate with one another (more details below) and getting college applications started.

By the time I had ironed out all of the problems I had about a week and a half left to code. I began by trying to implement a syntax based approach, using word "type" information I had gotten from Princeton's Word Net and a .Net API to get an idea of what words were what. This approach, inevitably, met a dead end. The more syntax analysis I attempted to plan out, the more I realized I needed to, essentially, teach the computer the English language. There is no way I could do this, I had neither the time nor the resources to complete such a monumental task. In the beginning of the summer I spent a week visiting colleges on the East coast, and while visiting one of the bookstores I purchased a text book on AI, entitled Artificial Intelligence a Modern Approach by Stuart Russell and Peter Norvig. I turned to this for guidance, looking through the section on natural language processing. I noticed a large paragraph focussed on an AI entitled ASKMSR. The AI answered questions by using wikipedia queries and comparing the summaries of query results from Google. I had an epiphany; what if I did something similar for my Question Getter? The wikipedia API offered a request to get, what were called, Backlinks, which were a list of articles which referenced the current article. This was my Google query. I had the idea to grab the articles which linked to the article which I was currently working on and search for the connections between the two in the backlink (specifically looking for "noun verb noun" connections). Just as I began to implement the core of the AI, summer ended. I was sucked back into the vortex that is AP classes, Speech and Debate and now College Applications.

AI Operation

I split the AI into two sections, what I like to call the "front end" and the "back end". The "front end" consists of the C# code which will grab the articles from the Wikipedia API and then parse them. By parsing them the "front end" gets rid of all extraneouse XML info (as the articles are grabbed in an XML format), identifies which words have a links to other articles (what I like to call "hrefs", which is from the XML syntax for a hyperlink) and keeps that "href" tag near the word. The "front end" then uses an API for Princeton's Word Net which get the "types" for each individual word (noun, verb, adjective etc.) placing the type in parentheses next to the word.The front end then transfers the parsed article to the "back end" using FTP. I was running my Lisp code on a Ubuntu Linux desktop and my C# on my Vista laptop so I used FTP to pass queries from the AI and results from the "front end" back and forth. The AI then takes the articles, which are stored in text files, puts them into local memory and runs through them, seperating the words and setting up lists to make AI analysis easier (i.e. each word is in a list with its "type" and "href" if it has a reference). The AI would then use the list of words and info to send queries for backlinks, searching the backlink article for information that is similar to the first article, ultimately determining what information is "important" based off of the results. Afterwords, the "back end" would return the results to the "front end" which would then display them in a windows form and save them to a file.